{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Node:\n",
    "\tdef __init__(self, num_actions, env):\n",
    "\t\tself.regret_sum = np.zeros(num_actions)\n",
    "\t\tself.strategy = np.zeros(num_actions)\n",
    "\t\tself.strategy_sum = np.zeros(num_actions)\n",
    "\t\tself.num_actions = num_actions\n",
    "\t\tself.number = 1\n",
    "\n",
    "\t\t# game information\n",
    "\t\tself.env = env\n",
    "\t\n",
    "\tdef get_strategy(self):\n",
    "\t\tnormalizing_sum = 0\n",
    "\t\tprob = np.random.dirichlet(alpha=[1] * self.num_actions)\n",
    "\t\tfor a in range(self.num_actions):\n",
    "\t\t\tif self.regret_sum[a] > 0:\n",
    "\t\t\t\tself.strategy[a] = self.regret_sum[a]\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.strategy[a] = 0\n",
    "\t\t\tnormalizing_sum += self.strategy[a]\n",
    "\n",
    "\t\tfor a in range(self.num_actions):\n",
    "\t\t\tif normalizing_sum > 0:\n",
    "\t\t\t\tself.strategy[a] /= normalizing_sum\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.strategy[a] = prob[a]\n",
    "\n",
    "\t\treturn self.strategy\n",
    "\n",
    "\tdef get_average_strategy(self):\n",
    "\t\tavg_strategy = np.zeros(self.num_actions)\n",
    "\t\tnormalizing_sum = 0\n",
    "\t\tfor action in range(self.num_actions):\n",
    "\t\t\tnormalizing_sum += self.strategy_sum[action]\n",
    "\t\tfor action in range(self.num_actions):\n",
    "\t\t\tif normalizing_sum > 0:\n",
    "\t\t\t\tavg_strategy[action] = self.strategy_sum[action] / normalizing_sum\n",
    "\t\t\telse:\n",
    "\t\t\t\tavg_strategy[action] = 1.0 / self.num_actions\n",
    "\t\t\n",
    "\t\treturn avg_strategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from GameModel.TempleOfHorror import TempleOfHorror\n",
    "\n",
    "\n",
    "class TempleCFR():\n",
    "\tdef __init__(self, iterations, nodes):\n",
    "\t\tself.iterations = iterations\n",
    "\t\tself.nodes = nodes\n",
    "\t\tself.env = TempleOfHorror(3)\n",
    "\t\tself.env_aux = self.env\n",
    "\t\tself.acting_player = random.randint(0,self.env.N-1)\n",
    "\n",
    "\n",
    "\tdef cfr_iterations_external(self):\n",
    "\t\tutility = np.zeros(self.env.N)\n",
    "\t\tfor t in tqdm(range(1, self.iterations + 1)):\n",
    "\t\t\tobservation = self.env.reset() \n",
    "\t\t\tfor player in range(self.env.N): # Players\n",
    "\t\t\t\tinfoSet = self.env.create_state(player, observation)\n",
    "\n",
    "\t\t\t\t#random.shuffle(self.cards)\n",
    "\t\t\t\tutility[player] += self.external_cfr(str(infoSet),  player,  self.acting_player, t)\n",
    "\t\t\t\t#print(player, utility[player])\n",
    "\n",
    "\t\tprint('Average game value: {}'.format(utility[0]/(self.iterations)))\n",
    "\t\t#for i in sorted(self.nodes):\n",
    "\t\t\t#print(i, self.nodes[i].get_average_strategy())\n",
    "\t\t\t\t\n",
    "\t  \n",
    "\n",
    "\tdef external_cfr(self, infoSet, learning_player, acting_player, t):\n",
    "\t\t#print('THIS IS ITERATION', t)\n",
    "\t\t#print(\"agent playing\", acting_player)\n",
    "\n",
    "\n",
    "\t\t#infoset = str(cards[acting_player]) + str(history) # infoset are card acting player can see and history\n",
    "\t\tif infoSet not in self.nodes:\n",
    "\t\t\tself.nodes[infoSet] = Node(len(self.env.action_spaces[f\"agent_{acting_player}\"]), copy.deepcopy(self.env_aux))\n",
    "\t\telse:\n",
    "\t\t\tself.nodes[infoSet].number += 1\t\n",
    "\n",
    "\n",
    "\n",
    "\t\tdone, winner = self.nodes[infoSet].env.referee()\n",
    "\n",
    "\t\t# History is in a terminal state then calculate payments\n",
    "\t\tif done:\t\t\t\n",
    "\t\t\tif self.nodes[infoSet].env.enc_player_role[f\"agent_{learning_player}\"] == winner:\n",
    "\t\t\t\treturn 100\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn -100\n",
    "\n",
    "\t\t#print(\"state\",  self.nodes[infoSet].env.observation_spaces)\t\t\n",
    "\t\t#print(\"state\",  self.nodes[infoSet].env.score)\t\n",
    "\n",
    "\t\t# Here is where self play is done\n",
    "\t\tif acting_player == learning_player:\n",
    "\t\t\taction_space_length = len(self.nodes[infoSet].env.action_spaces[f\"agent_{acting_player}\"])\n",
    "\t\t\tutility = np.zeros(action_space_length) \n",
    "\t\t\tnode_utility = 0\n",
    "\t\t\tstrategy = self.nodes[infoSet].get_strategy()\n",
    "\n",
    "\t\t\tfor index, action in enumerate(self.nodes[infoSet].env.action_spaces[f\"agent_{acting_player}\"]):\n",
    "\t\t\t\tnext_acting_player = action\n",
    "\n",
    "\t\t\t\tdone, next_observation_spaces, _ = self.nodes[infoSet].env.step(action)\n",
    "\t\t\t\tnextInfoSet = self.nodes[infoSet].env.create_state(next_acting_player, next_observation_spaces)\n",
    "\n",
    "\t\t\t\tself.env_aux = copy.deepcopy(self.nodes[infoSet].env)\n",
    "\t\t\t\tutility[index] = self.external_cfr(str(nextInfoSet), learning_player, next_acting_player,t)\n",
    "\t\t\t\t\n",
    "\t\t\t\tnode_utility += strategy[index] * utility[index]\n",
    "\n",
    "\t\t\tfor action in range(action_space_length):\n",
    "\t\t\t\tregret = utility[action] - node_utility\n",
    "\t\t\t\tself.nodes[infoSet].regret_sum[action] = regret\n",
    "\t\t\t\t#print(regret)\n",
    "\n",
    "\t\t\treturn node_utility\n",
    "\n",
    "\t\telse: #acting_player != learning_player sample strategy\n",
    "\t\t\taction_space_length = len(self.nodes[infoSet].env.action_spaces[f\"agent_{acting_player}\"])\n",
    "\t\t\tstrategy = self.nodes[infoSet].get_strategy()\n",
    "\t\t\tutility = 0\n",
    "\t\t\taction = np.random.choice(self.nodes[infoSet].env.action_spaces[f\"agent_{acting_player}\"], p=strategy)\n",
    "\n",
    "\t\t\tnext_acting_player = action\n",
    "\t\t\t#print(\"other\")\n",
    "\t\t\tdone, next_observation_spaces, _ = self.nodes[infoSet].env.step(action)\n",
    "\t\t\tnextInfoSet = self.nodes[infoSet].env.create_state(next_acting_player, next_observation_spaces)\n",
    "\t\t\t\n",
    "\t\t\tself.env_aux = copy.deepcopy(self.nodes[infoSet].env)\n",
    "\t\t\tutility = self.external_cfr(str(nextInfoSet), learning_player, next_acting_player,t)\n",
    "\n",
    "\t\tfor index_action in range(action_space_length):\n",
    "\t\t\tself.nodes[infoSet].strategy_sum[index_action] += strategy[index_action]\n",
    "\n",
    "\t\t\treturn utility\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 593.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game value: 48.905885545657554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tk = TempleCFR(1000, {})\n",
    "\tk.cfr_iterations_external()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
